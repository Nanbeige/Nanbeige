<!-- markdownlint-disable first-line-h1 -->

<!-- markdownlint-disable html -->

<div align="center">
<h1>
  Nanbeige-16B
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/Nanbeige/" target="_blank">Hugging Face</a>
</p>

<h4 align="center">
    <p>
        <a href="https://github.com/Nanbeige/Nanbeige/blob/main/README.md">ä¸­æ–‡</a> |
        <b>English</b>
    <p>
</h4>


# <span id="Introduction">Model Introduction</span>

Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.

The Base-32k version is based on the Nanbeige-16B-Base model, which uses YaRN interpolation method to modify the position encoding, and performs full parameter incremental pre-training with 20 billion tokens of Chinese, English, and code corpora, under 32k max length.

The Chat version and Chat-32k version are based on the Nanbeige-16B-Base model and Nanbeige-16B-Base-32k model respectively. They have undergone extensive human-aligned training.

# <span id="Evaluation">Performance Evaluation</span>

We selected datasets such as C-Eval, CMMLU, MMLU, GSM8K, HumanEval, BBH, MBPP, to evaluate the capabilities of the Base model. Among open-source models of similar scale, it achieved relatively good performance.

###
| Model              | C-Eval | CMMLU | MMLU  | GSM8K | HumanEval | BBH   | MBPP  |
|--------------------|--------|-------|-------|-------|-----------|-------|-------|
| LLaMA2-13B         | 35.80  | 38.40 | 54.80 | 29.60 | 20.10     | 45.62 | 26.80 |
| Baichuan2-13B-Base | 58.10  | 61.30 | 59.17 | 52.77 | 17.07     | 48.98 | 30.80 |
| Qwen-14B           | 72.10  | 70.2  | 66.30 | 61.30 | 32.30     | 53.40 | 39.80 |
| InternLM-20B       | 58.80  | 59    | 62.05 | 52.62 | 25.61     | 52.51 | 35.60 |
| XVERSE-13B         | 53.70  | 59.1  | 55.21 | 18.20 | 15.85     | 38.06 | -     |
| Skywork-13B        | 60.60  | 61.8  | 62.10 | 55.80 | -         | -     | -     |
| Nanbeige-16B-Base  | 63.80  | 66.58 | 64.80 | 57.62 | 24.56     | 50.68 | 36.40 |


### C-Eval Scores
|                   | Avg   | Avgï¼ˆHardï¼‰ | STEM | Social Sciences | Humanities | Other  |
|-------------------|------|----------|------|------|------|------|
| Chinese-LLaMA-13B | 33.3 | 27.3     | 31.6 | 37.2 | 33.6 | 32.8 |
| Baichuan-13B      | 53.6 | 36.7     | 47.0 | 66.8 | 57.3 | 49.8 |
| Qwen-14B          | 72.1 | 53.7     | 65.7 | 85.4 | 75.3 | 68.4 |
| XVERSE-13B        | 54.7 | 33.5     | 45.6 | 66.2 | 58.3 | 56.9 |
| Skywork-13B       | 60.6 | 39.4     | 51.2 | 74.6 | 67.8 | 57.5 |
| Nanbeige-16B-Base | 63.8 | 43.5     | 57.8 | 77.2 | 66.9 | 59.4 |

### Long Context Understanding Base Model
We evaluated the Nanbeige-16B-Base-32k model using LSHT, LCC, and Multifiled_QA_ZH from LongBench datasets. Compared to the Base model of the same parameter size with long-context comprehension capabilities, it achieved impressive results.

|                            |  LSHT  |  LCC  | Multifiled_QA_ZH |
|----------------------------|--------|-------|------------------|
| Chinese-Llama2-13B-16k     |  31.0  |  9.6  |       36.4       |
| Qwen-14B-Dynamnic_ntk-Logn |  16.0  |  66.7 |       30.0       | 
| Nanbeige-16B-Base-32k      |  40.3  |  70.7 |       47.2       |

### Long Context Understanding Chat Model
We evaluated the Nanbeige-16B-Chat-32k model using LongBench datasets. Compared to the Chat model of the same parameter size with long-context comprehension capabilities, it achieved impressive results.

|                          | Average | Single-Doc QA | Multi-Doc QA | Summarization | Few-shot | Synthetic | Code |
|--------------------------|---------|---------------|--------------|---------------|----------|-----------|------|
| BlueLM-7B-Chat-32K       |  41.2   |     35.6      |     36.2     |     18.8      |   56.9   |   47.6    | 52.8 |
| Chatglm2-6B-32k          |  41.5   |     37.6      |     34.6     |     24.7      |   51.3   |   47.6    | 54.2 |
| Chatglm3-6B-32k          |  50.2   |     45.8      |     46.1     |     26.6      |   61.2   |   65.0    | 56.2 |
| Chinese-Alpaca-2-13B-16K |  29.7   |     47.9      |     26.7     |     13.0      |   22.3   |   21.5    | 46.6 |
| Ziya-Reader-13B-v1.0     |    \    |       \       |     42.8     |     15.3      |     \    |   66.0    |   \  |
| Nanbeige-16B-Chat-32k    |  52.3   |     48.9      |     41.1     |     26.3      |   63.3   |   66.8    | 67.5 |


### LLMEval-3
We conducted a comprehensive evaluation of Nanbeige-16B-Chat model based on **LLMEval-3** ( [Github](https://github.com/llmeval/llmeval-3) / [Homepage](http://llmeval.com/index) ), and the results are as follows:

| æ¨¡å‹åç§°    | ç›¸å¯¹åˆ†æ•°-GPT4 | ç›¸å¯¹åˆ†æ•°-GPT3.5 | ç»å¯¹åˆ†æ•°  | å·¥å­¦   | ç»æµå­¦  | æ•™è‚²å­¦  | æ³•å­¦   | æ–‡å­¦   | ç®¡ç†å­¦  | ç†å­¦   | å†å²å­¦  | åŒ»å­¦   | å†›äº‹å­¦  |
|----------------------------|-----------|-------------|-------|------|------|------|------|------|------|------|------|------|------|
| Baidu3.5                   | 104.21    | 121.39      | 77.53 | 8.13 | 8.00 | 8.63 | 7.97 | 6.23 | 7.63 | 7.33 | 8.77 | 7.47 | 7.37 |
| ChatGLM-pro                | 103.45    | 120.51      | 76.97 | 6.97 | 8.47 | 7.97 | 8.93 | 7.23 | 7.70 | 6.33 | 8.37 | 7.13 | 7.87 |
| GPT-4                      | 100.00    | 116.49      | 74.40 | 7.23 | 7.80 | 7.73 | 8.40 | 6.73 | 7.67 | 7.73 | 7.07 | 6.20 | 7.83 |
| **Nanbeige-16B-Chat**               | 94.26     | 109.80      | 70.13 | 6.00 | 7.87 | 8.20 | 8.33 | 6.07 | 6.83 | 6.00 | 7.80 | 5.80 | 7.23 |
| minimax-abab5              | 93.28     | 108.66      | 69.40 | 5.83 | 7.50 | 7.77 | 8.37 | 6.40 | 6.33 | 5.07 | 8.33 | 5.93 | 7.87 |
| Baichuan2-13B-Chat         | 92.91     | 108.23      | 69.13 | 6.00 | 7.53 | 8.63 | 8.13 | 6.23 | 6.33 | 5.63 | 8.20 | 5.43 | 7.00 |
| Qwen-14B-Chat              | 86.33     | 100.56      | 64.23 | 5.77 | 7.07 | 7.07 | 7.37 | 5.70 | 6.20 | 5.93 | 6.97 | 5.40 | 6.77 |
| GPT-3.5-turbo              | 85.84     | 100.00      | 63.87 | 6.27 | 6.87 | 7.23 | 7.40 | 5.40 | 6.30 | 6.37 | 6.00 | 5.17 | 6.87 |
| ChatGLM2-6B                | 75.71     | 88.19       | 56.33 | 4.03 | 6.33 | 7.00 | 7.57 | 4.77 | 5.93 | 4.23 | 5.87 | 5.07 | 5.53 |
| ziya_v1.1-13b              | 70.20     | 81.78       | 52.23 | 4.67 | 5.77 | 6.07 | 6.53 | 4.53 | 5.33 | 3.70 | 5.00 | 4.63 | 6.00 |
| BELLE-Llama2-13B-chat-0.4M | 68.82     | 80.16       | 51.20 | 4.47 | 5.93 | 6.20 | 6.77 | 4.33 | 4.97 | 4.10 | 5.07 | 3.77 | 5.60 |
| Linly-Chinese-LLaMA2-13B   | 67.82     | 79.00       | 50.46 | 3.87 | 5.80 | 5.83 | 6.57 | 3.93 | 5.37 | 4.07 | 5.43 | 3.93 | 5.67 |
| InternLM-Chat-7B           | 61.06     | 71.13       | 45.43 | 3.83 | 5.13 | 5.27 | 6.57 | 3.90 | 4.83 | 3.10 | 4.87 | 3.67 | 4.27 |
| Llama-2-7b-chat-hf         | 51.11     | 59.54       | 38.03 | 3.33 | 4.77 | 3.77 | 5.03 | 3.07 | 3.77 | 3.93 | 4.00 | 2.40 | 3.97 |



# <span id="Inference">Inference</span>

## Requirements

- python 3.8 and above
  
- transformers 4.33.3
  
- pytorch 2.0 and above

You can install the dependent libraries with the following pip command

```
pip install transformers==4.33.3 transformers_stream_generator deepspeed einops==0.3.2 datasets==2.10.0
```

## Inference Code

The following code can be used to invoke the model for text generation:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
tokenizer = AutoTokenizer.from_pretrained("Nanbeige/Nanbeige-16B-Base", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Nanbeige/Nanbeige-16B-Base", device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True)

inputs = tokenizer('ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬\nå¾·å›½çš„é¦–éƒ½æ˜¯æŸæ—\nå­ŸåŠ æ‹‰å›½çš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs, max_new_tokens=64)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬\nå¾·å›½çš„é¦–éƒ½æ˜¯æŸæ—\nå­ŸåŠ æ‹‰å›½çš„é¦–éƒ½æ˜¯è¾¾å¡\nå·´è¥¿çš„é¦–éƒ½æ˜¯å·´è¥¿åˆ©äºš\nå°åº¦çš„é¦–éƒ½æ˜¯æ–°å¾·é‡Œ\næ³•å›½çš„é¦–éƒ½æ˜¯å·´é»\nç¾å›½çš„é¦–éƒ½æ˜¯åç››é¡¿\næ—¥æœ¬çš„é¦–éƒ½æ˜¯ä¸œäº¬\nä¿„ç½—æ–¯çš„é¦–éƒ½æ˜¯è«æ–¯ç§‘\næ¾³å¤§åˆ©äºšçš„é¦–éƒ½æ˜¯å ªåŸ¹æ‹‰\nåŠ æ‹¿å¤§çš„é¦–éƒ½æ˜¯æ¸¥å¤ªå
```


# <span id="Demo">Chat Demo</span>

### CLI Demo
We provide a CLI demo example in cli_demo.py, which supports streaming output for the generation. Users can interact with Nanbeige-16B-Chat by inputting prompts, and the model returns model outputs in the streaming mode. Run the command below:

```
python cli_demo.py --cpu # inference cpu only
```

# <span id="Finetuning">Finetuning</span>
We provide a sample training script that utilizes DeepSpeed for training. Users need to ensure that the training environment has DeepSpeed installed and modify the config/hostfile based on the cluster setup. If using multiple machines and multiple GPUs, the IP configuration in SSH needs to be modified for each node. For more details, please refer to the official DeepSpeed documentation. To start training, execute the scripts/train.sh script.

```
scripts/train.sh
```

The training data file is in JSONL format, with each line representing a JSON-formatted sample. Each sample is a dict that includes an ID and a list of dialogues. Our training script by default reads the train.jsonl and eval.jsonl files located in the data/ directory. Here is an example of a simple sample:

``` json
{
  "id": 312,
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèªæ˜ä¸”åšå­¦ã€ä¹äºåŠ©äººã€æ­æ•¬è€Œè¯šå®çš„åŠ©æ‰‹ã€‚"},
    {"role": "human", "content": "å¤ªé˜³è¡¨é¢çš„æ¸©åº¦æœ‰å¤šé«˜?"},
    {"role": "assistant", "content": "å¤ªé˜³è¡¨é¢çš„æ¸©åº¦éå¸¸é«˜ï¼Œçº¦ä¸ºçº¦ 5,500Â°Cï¼ˆ9,932Â°Fï¼‰å·¦å³ã€‚è¿™ä¸ªæ¸©åº¦æ˜¯é€šè¿‡å¤ªé˜³çš„å…‰è°±ç‰¹å¾å’Œå…¶ä»–è§‚æµ‹æ•°æ®æµ‹å¾—çš„ã€‚å¤ªé˜³çš„å¤–éƒ¨å¤§æ°”å±‚ç§°ä¸ºå…‰çƒï¼Œæ˜¯æˆ‘ä»¬èƒ½å¤Ÿç›´æ¥è§‚å¯Ÿåˆ°çš„éƒ¨åˆ†ï¼Œå®ƒçš„æ¸©åº¦éå¸¸é«˜ã€‚ç„¶è€Œï¼Œå¤ªé˜³çš„å†…éƒ¨æ¸©åº¦è¦æ›´é«˜ï¼Œä¸­å¿ƒæ ¸å¿ƒçš„æ¸©åº¦ä¼°è®¡çº¦ä¸º 15,000,000Â°Cï¼ˆ27,000,000Â°Fï¼‰ã€‚è¿™äº›é«˜æ¸©æ˜¯ç”±å¤ªé˜³å†…éƒ¨çš„æ ¸èšå˜ååº”å¼•èµ·çš„ï¼Œæ ¸èšå˜å°†æ°¢è½¬åŒ–ä¸ºæ°¦å¹¶é‡Šæ”¾å‡ºå·¨å¤§çš„èƒ½é‡ã€‚"},
    {"role": "human", "content": "é‚£é‡‘æ˜Ÿå‘¢"},
    {"role": "assistant", "content": "é‡‘æ˜Ÿæ˜¯å¤ªé˜³ç³»ä¸­æœ€æ¥è¿‘å¤ªé˜³çš„è¡Œæ˜Ÿä¹‹ä¸€ï¼Œå› æ­¤å®ƒçš„è¡¨é¢æ¸©åº¦éå¸¸é«˜ï¼Œæ˜¯æ•´ä¸ªå¤ªé˜³ç³»ä¸­æœ€é«˜çš„ä¹‹ä¸€ã€‚é‡‘æ˜Ÿçš„è¡¨é¢æ¸©åº¦çº¦ä¸ºçº¦ 462Â°Cï¼ˆ864Â°Fï¼‰ã€‚è¿™ä¸ªé«˜æ¸©æ˜¯ç”±äºé‡‘æ˜Ÿçš„å¤§æ°”å±‚ä¸­å«æœ‰å¤§é‡çš„æ¸©å®¤æ°”ä½“ï¼Œå¦‚äºŒæ°§åŒ–ç¢³ï¼Œå¯¼è‡´æ¸©å®¤æ•ˆåº”çš„åŠ å¼ºã€‚é‡‘æ˜Ÿçš„å¤§æ°”å±‚ä¸­çš„æ¸©å®¤æ•ˆåº”ä½¿å¾—çƒ­é‡æ— æ³•é€ƒé€¸ï¼Œå¯¼è‡´è¡¨é¢æ¸©åº¦å¼‚å¸¸å‡é«˜ã€‚é‡‘æ˜Ÿçš„é«˜æ¸©å’Œæé«˜çš„å¤§æ°”å‹ä½¿å¾—é‡‘æ˜Ÿè¡¨é¢æ¡ä»¶æä¸ºæ¶åŠ£ï¼Œè¡¨é¢è¦†ç›–ç€ç†”å²©å¹³åŸå’Œç«å±±ï¼Œæ²¡æœ‰æ¶²æ€æ°´å­˜åœ¨ã€‚è¿™ä½¿å¾—é‡‘æ˜Ÿæˆä¸ºä¸€ä¸ªç±»ä¼¼åœ°çƒçš„ç‚™çƒ­åœ°ç‹±ï¼Œæ— æ³•æ”¯æŒç”Ÿå‘½å­˜åœ¨ã€‚"},
  ]
}
```

# <span id="Limitations">Limitations</span>
While we place great emphasis on the safety of the model during the training process, striving to ensure that its outputs align with ethical and legal requirements, it may not completely avoid generating unexpected outputs due to the model's size and probabilistic nature. These outputs may include harmful content such as bias or discrimination. Please don't propagate such content. We do not assume any responsibility for the consequences resulting from the dissemination of inappropriate information.

# <span id="License">License</span>
When using the Nanbeige models, you must comply with the Apache 2.0 License and the Nanbeige Models Community License Agreement. If you intend to use the Nanbeige Models or its derivatives for commercial purposes, please submit application materials to meet the requirements of the Nanbeige Models Community License Agreement by contacting nanbeige@126.com. After review, We will grant you a non-exclusive, worldwide, non-transferable, non-sublicensable and revocable commercial copyright license.

## Citation
If you find our work useful, please cite our project:
```
@misc{NanBeiGe,
  title = {NanBeiGe LLM},
  author = {NanBeiGe LLM Lab},
  howpublished = {\url{https://github.com/Nanbeige/Nanbeige)},
  year = {2023},
  month = {Nov},
}
```
